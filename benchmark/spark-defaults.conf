spark.jars.ivy /tmp/.ivy
spark.driver.bindAddress=0.0.0.0
spark.driver.port 10060
spark.driver.blockManager.port 10061

spark.armada.pod.labels foo=bar

spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.committer.name=file
spark.hadoop.fs.s3a.multipart.threshold=320M
spark.hadoop.fs.s3a.multipart.size=32M
spark.hadoop.fs.s3a.connection.maximum=10000
spark.hadoop.fs.s3a.threads.max=5000
spark.hadoop.fs.s3a.path.style.access True

spark.eventLog.enabled false
spark.armada.driver.limit.memory 6000Mi
spark.armada.driver.request.memory 6000Mi
spark.armada.executor.limit.memory 6000Mi
spark.armada.executor.request.memory 6000Mi
spark.armada.executor.limit.cores 5
spark.armada.executor.request.cores 5

spark.local.dir=/tmp

spark.io.encryption.enabled=false
spark.lineage.enabled=true


# spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.sql.shuffle.partitions=1000

spark.ui.enabled=true
spark.ui.killEnabled=true

# shuffle defaults
spark.shuffle.file.buffer=1m
spark.shuffle.io.backLog=8192
spark.shuffle.io.serverThreads=128
spark.shuffle.registration.maxAttempts=5
spark.shuffle.registration.timeout=30000
spark.shuffle.service.index.cache.size=2048m
spark.shuffle.unsafe.file.output.buffer=5m
spark.io.compression.lz4.blockSize=128kb
spark.unsafe.sorter.spill.reader.buffer.size=1m
spark.dynamicAllocation.shuffleTracking.enabled=false

spark.sql.legacy.parquet.nanosAsLong=true

# dynamic allocation thresholds
spark.dynamicAllocation.enabled=false
spark.dynamicAllocation.executorIdleTimeout=60
spark.dynamicAllocation.schedulerBacklogTimeout=1
spark.dynamicAllocation.cachedExecutorIdleTimeout=240
spark.dynamicAllocation.initialExecutors=2
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=1


# spark.submit.deployMode=client

# default optimization
spark.sql.autoBroadcastJoinThreshold=26214400
spark.executorEnv.MKL_NUM_THREADS=1
spark.yarn.appMasterEnv.OPENBLAS_NUM_THREADS=1
spark.executorEnv.OPENBLAS_NUM_THREADS=1
spark.rss.estimate.task.concurrency.dynamic.factor=1.0
spark.rss.client.io.compression.codec=lz4
spark.sql.execution.arrow.pyspark.enabled=true

## View ACL's
spark.acls.enable=true
#spark.ui.view.acls.groups=XXX


# # Uniffle config
# # this needs to be disabled when using an external shuffle service
# spark.shuffle.service.enabled=false
# #spark.shuffle.manager=org.apache.spark.shuffle.RssShuffleManager
# spark.rss.coordinator.quorum=XXX
# spark.rss.estimate.task.concurrency.dynamic.factor=1.0
# spark.sql.adaptive.localShuffleReader.enabled=false
# spark.rss.client.shuffle.data.distribution.type=LOCAL_ORDER
# spark.rss.client.unregister.request.timeout.sec=60000
# spark.rss.client.unregister.thread.pool.size=120
# # make uniffle aware that the spark cluster is preemptible
# spark.rss.cluster.preemptible=true

spark.ui.prometheus.enabled=true
spark.executor.processTreeMetrics.enabled=true
spark.metrics.appStatusSource.enabled=true
spark.scheduler.listenerbus.eventqueue.capacity=100000

## UI config
spark.ui.retainedJobs=999999
spark.ui.retainedStages=999999
spark.ui.retainedTasks=999999
spark.worker.ui.retainedExecutors=999999
spark.sql.ui.retainedExecutions=999999
spark.ui.retainedDeadExecutors=999999
spark.ui.timeline.executors.maximum=999999
#spark.ui.custom.executor.log.url=XXX
#spark.ui.custom.driver.log.url=XXX

## Kubernetes
spark.kubernetes.driver.label.spark=true
spark.kubernetes.container.image.pullPolicy=IfNotPresent
spark.kubernetes.file.upload.path=hdfs:///tmp/
spark.executorEnv.KRB5_TRACE=/dev/stdout
spark.kubernetes.allocation.maxPendingPods=30

## Custom Kubernetes
spark.kubernetes.ffs.probe.enabled=true


## S3 Configuration
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.committer.name=directory
spark.hadoop.fs.s3a.multipart.threshold=320M
spark.hadoop.fs.s3a.multipart.size=32M
spark.hadoop.fs.s3a.connection.maximum=10000
spark.hadoop.fs.s3a.threads.max=5000
#spark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter
#spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol

## override the below to allow allSilver users to view logs in elk
spark.kubernetes.driver.label.spark-logs-secure=true
spark.kubernetes.executor.label.spark-logs-secure=true

spark.driver.cores=2
spark.driver.memory=18g

spark.executor.cores=5
spark.executor.memory=46g



spark.kubernetes.executor.pod.featureSteps=org.apache.spark.deploy.k8s.features.ShuffleServiceExecutorFeatureStep
spark.blockManager.port=7337

## Fallback storage migration
spark.decommission.enabled=true
spark.storage.decommission.enabled=true
spark.storage.decommission.shuffleBlocks.enabled=true
spark.storage.decommission.shuffleBlocks.maxDiskSize=0
spark.storage.decommission.replicationReattemptInterval=100ms
spark.storage.decommission.fallbackStorage.cleanUp=false
spark.storage.decommission.fallbackStorage.replicationDelay=30s
spark.storage.decommission.fallbackStorage.replicationWait=1s


spark.armada.executor.trackerTimeout=36000s
